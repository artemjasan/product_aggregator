# Backend part of Product aggregator
REST API JSON Python microservice which allows users to browse a product catalog and which automatically
updates prices from the offer service.

## Start here
This is the `api` part of the `Product Aggregator`.

For that project I've used the following technologies:
- [Python 3.10](https://www.python.org/downloads/release/python-3104/)
- [Poetry](https://python-poetry.org/) modern and powerful package manager
- [Django](https://www.djangoproject.com/download/) & [Django Rest Framework](https://www.django-rest-framework.org/)
for creation API layer
- [Postgres](https://www.postgresql.org/) main project database
- [Docker]() & [Docker Compose]() Containerized development tools
- [Pytest](https://docs.pytest.org/en/7.1.x/) modern, flexible python framework for testing
- [Celery](https://docs.celeryq.dev/en/stable/getting-started/first-steps-with-celery.html) task queue and background worker
- [Redis](https://redis.io/) broker for Celery
- [Flower](https://flower.readthedocs.io/en/latest/) Celery monitoring tool
- [pre-commit](https://pre-commit.com/) additional version control tool

It's start of your journey on this project.
For convenience, I tried to use the features provided by containerization using `Docker` and
`Docker Compose`. They can greatly isolate the development process and unify the development result across all dev's
machines, but before using `Docker` you have to do a few steps:

**Install poetry package manager to your machine**
```shell
$ pip install poetry
```
**Activate poetry**
```shell
$ poetry shell
```
**Install dependencies to venv**
```shell
$ poetry install
```
## Launch application by `Docker`/`Docker Compose`
Firstly you need to create a `local.env` file in root directory. It can be empty, because by default,
Django uses standard Base URL. But, if you want to configure you custom URL, just add a new environment
variable `BASE_OFFER_MICROSERVICE_API` with your Base URL.

Next step is build Docker image:
#### Via `docker-compose`
```shell
$ docker-compose build
$ docker-compose up --no-start
$ docker-compose run --rm web python aggregator_project/manage.py migrate
$ docker-compose up
```
#### Via `make`
```shell
$ make first-launch
```

## How to use the application
> Firstly you should register a new user and login, because the application
> has basic authentication system, which based on JWT:

**Registration**

`http://0.0.0.0:8000/api/auth/registration/`

**Login**

`http://0.0.0.0:8000/api/auth/login/`

### Application Routes
Swagger was used for autogenerated of documentation. You can find everything there:

**Autogenerated documentation**

`http://0.0.0.0:8000/docs/swagger/`
`http://0.0.0.0:8000/docs/redoc/`


## Testing
To run the tests I also use the containerization provided by `Docker`.
#### Via `docker-compose`
```shell
$ docker-compose run --rm web pytest
```
after the tests pass, it is necessary to stop the work of containers.
```shell
$ docker-compose stop
```
#### Via `make`
```shell
$ make test
```

## Additional tools
**pre-commit and other checkers (flake8, mypy, isort, black)**
> Before starting, you have to install the git hook scripts:
```shell
$ pre-commit install
```
Now pre-commit will run automatically on git commit, or you can
launch it manually:
```shell
$ pre-commit run --all-files
```
**Flower**

Flower is a web based tool for monitoring and administrating Celery clusters.
Visit it at http://0.0.0.0:5555
> If when you click on an existing worker, you get the message unknown worker, just refresh the page

## What could also be added to the project
There are hundreds of things that can be added/updated:

- Unique access token for each user, load process replace to Celery
- More complex authentication system for users and their own products
- Use Redis for extra exercises (view caching)
- Optimization process of creation product offers, add update process
- Add registration process to Celery with subsequent check of result
- Add track the history of offer prices and create an endpoint which returns by using `django-simple-history`
- Deploy to Heroku, in future to AWS
